{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25da67cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5d2602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35e941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome ,to Krish Naik's NLP Tutorials.\n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98a61a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome ,to Krish Naik's NLP Tutorials.\n",
      "Please do watch the entire course! to become expert in NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837be248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Welcome ,to Krish Naik's NLP Tutorials.\",\n",
       " 'Please do watch the entire course!',\n",
       " 'to become expert in NLP.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenization \n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "documents=sent_tokenize(corpus)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5653d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenization \n",
    "\n",
    "## Paragraph--> Words\n",
    "## sentences--> words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words=word_tokenize(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e082ce5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorials.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973771d7",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or roots of words known as lemma.\n",
    "\n",
    "Eating, eat, eaten --> eat\n",
    "going,goes,gone --> go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512bbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['Eating', 'eat', 'eaten','going','goes','gone','writing','programming','programs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6643090",
   "metadata": {},
   "source": [
    "## PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "615607bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating---->eat\n",
      "eat---->eat\n",
      "eaten---->eaten\n",
      "going---->go\n",
      "goes---->goe\n",
      "gone---->gone\n",
      "writing---->write\n",
      "programming---->program\n",
      "programs---->program\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemming=PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(word+\"---->\"+stemming.stem(word))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69699712",
   "metadata": {},
   "source": [
    "## RegexpStemmer class\n",
    "It basically takes single regular expression and removes any prefix and suffix that matches the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d72ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "reg_stemmer=RegexpStemmer('ing$|s$|able$',min=4)\n",
    "\n",
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76e75d",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b531ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating---->eat\n",
      "eat---->eat\n",
      "eaten---->eaten\n",
      "going---->go\n",
      "goes---->goe\n",
      "gone---->gone\n",
      "writing---->write\n",
      "programming---->program\n",
      "programs---->program\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball=SnowballStemmer('english')\n",
    "\n",
    "for word in words:\n",
    "    print(word+'---->'+snowball.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3943c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('fairly'),stemming.stem('sportingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7daf4803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('fairly'),snowball.stem('sportingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0786061",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization technique is like stemming.The output we will get after lemmatization is called lemma, which is root word rather than root stem,the output of stemming.After lemmatization we will be getting valid woprd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eed6005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77ed68e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Wordnet\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize('going',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad3bbdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating---->Eating\n",
      "eat---->eat\n",
      "eaten---->eat\n",
      "going---->go\n",
      "goes---->go\n",
      "gone---->go\n",
      "writing---->write\n",
      "programming---->program\n",
      "programs---->program\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f6a0e",
   "metadata": {},
   "source": [
    "##  Stopwords \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62774599",
   "metadata": {},
   "outputs": [],
   "source": [
    "Speech=\"\"\"PM Hegde. Okay. So, first virtue is generosity, the second virtue is Pure Ethics. Third is tolerance because you’ll be in the midst of pain, your doctor or nurses, they are always in midst of pain. That’s why tolerance.\n",
    "\n",
    "The fourth is Perseverance. A doctor never gives up. Never gives up. He will always say, I can save this situation or save, prevent the disease or cure the disease. Fifth is Cultivating Pure Concentration. When you see a patient, he should feel my doctor sees me. Sees me through, not just keep the thermoscope and get away. And the way you treat the patient, he feels in this, cultivating the pure concentration of the patient.\n",
    "\n",
    "The last virtue is Intelligent. That means no field in the country, in any part of the world, it needs update. Because medical field and equipment, in diagnosis, in treatment, continuously, everyday its changing. Fortunately, internet is there. These virtues will empower the caregivers with a human heart. I’m sure the medical who are present here and elsewhere will have all these six virtues that will reinforce confidence of the citizen in the healthcare system.\n",
    "\n",
    "So, finally, in conclusion, I would to like say what I will be remembered for, what I will be remembered for. First, friends, I want to leave you with a thought today. What is the one action which will make you great? Every one of you has a page in the history of the world. What is that page? How do you make that page, which is going to be referred by the prosperity? There’s a need to give a vision to your ambition.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9517fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84021d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abd73640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28d76cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PM Hegde.',\n",
       " 'Okay.',\n",
       " 'So, first virtue is generosity, the second virtue is Pure Ethics.',\n",
       " 'Third is tolerance because you’ll be in the midst of pain, your doctor or nurses, they are always in midst of pain.',\n",
       " 'That’s why tolerance.',\n",
       " 'The fourth is Perseverance.',\n",
       " 'A doctor never gives up.',\n",
       " 'Never gives up.',\n",
       " 'He will always say, I can save this situation or save, prevent the disease or cure the disease.',\n",
       " 'Fifth is Cultivating Pure Concentration.',\n",
       " 'When you see a patient, he should feel my doctor sees me.',\n",
       " 'Sees me through, not just keep the thermoscope and get away.',\n",
       " 'And the way you treat the patient, he feels in this, cultivating the pure concentration of the patient.',\n",
       " 'The last virtue is Intelligent.',\n",
       " 'That means no field in the country, in any part of the world, it needs update.',\n",
       " 'Because medical field and equipment, in diagnosis, in treatment, continuously, everyday its changing.',\n",
       " 'Fortunately, internet is there.',\n",
       " 'These virtues will empower the caregivers with a human heart.',\n",
       " 'I’m sure the medical who are present here and elsewhere will have all these six virtues that will reinforce confidence of the citizen in the healthcare system.',\n",
       " 'So, finally, in conclusion, I would to like say what I will be remembered for, what I will be remembered for.',\n",
       " 'First, friends, I want to leave you with a thought today.',\n",
       " 'What is the one action which will make you great?',\n",
       " 'Every one of you has a page in the history of the world.',\n",
       " 'What is that page?',\n",
       " 'How do you make that page, which is going to be referred by the prosperity?',\n",
       " 'There’s a need to give a vision to your ambition.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "sentences=nltk.sent_tokenize(Speech) ## Tokenization process \n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09812ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pm hegd .',\n",
       " 'okay .',\n",
       " 'so , first virtu generos , second virtu pure ethic .',\n",
       " 'third toler ’ midst pain , doctor nurs , alway midst pain .',\n",
       " 'that ’ toler .',\n",
       " 'the fourth persever .',\n",
       " 'a doctor never give .',\n",
       " 'never give .',\n",
       " 'he alway say , i save situat save , prevent diseas cure diseas .',\n",
       " 'fifth cultiv pure concentr .',\n",
       " 'when see patient , feel doctor see .',\n",
       " 'see , keep thermoscop get away .',\n",
       " 'and way treat patient , feel , cultiv pure concentr patient .',\n",
       " 'the last virtu intellig .',\n",
       " 'that mean field countri , part world , need updat .',\n",
       " 'becaus medic field equip , diagnosi , treatment , continu , everyday chang .',\n",
       " 'fortun , internet .',\n",
       " 'these virtu empow caregiv human heart .',\n",
       " 'i ’ sure medic present elsewher six virtu reinforc confid citizen healthcar system .',\n",
       " 'so , final , conclus , i would like say i rememb , i rememb .',\n",
       " 'first , friend , i want leav thought today .',\n",
       " 'what one action make great ?',\n",
       " 'everi one page histori world .',\n",
       " 'what page ?',\n",
       " 'how make page , go refer prosper ?',\n",
       " 'there ’ need give vision ambit .']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply stopwords aand filter and then apply stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('English'))]\n",
    "    sentences[i]=' '.join(words) ## converting all the list of words into the sentences\n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34778408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pm hegd .',\n",
       " 'okay .',\n",
       " ', first virtu genero , second virtu pure ethic .',\n",
       " 'third toler ’ midst pain , doctor nur , alway midst pain .',\n",
       " '’ toler .',\n",
       " 'fourth persev .',\n",
       " 'doctor never give .',\n",
       " 'never give .',\n",
       " 'alway say , save situat save , prevent disea cure disea .',\n",
       " 'fifth cultiv pure concentr .',\n",
       " 'see patient , feel doctor see .',\n",
       " 'see , keep thermoscop get away .',\n",
       " 'way treat patient , feel , cultiv pure concentr patient .',\n",
       " 'last virtu intellig .',\n",
       " 'mean field countri , part world , need updat .',\n",
       " 'becaus medic field equip , diagnosi , treatment , continu , everyday chang .',\n",
       " 'fortun , internet .',\n",
       " 'virtu empow caregiv human heart .',\n",
       " '’ sure medic present elsewh six virtu reinforc confid citizen healthcar system .',\n",
       " ', final , conclus , would like say rememb , rememb .',\n",
       " 'first , friend , want leav thought today .',\n",
       " 'one action make great ?',\n",
       " 'everi one page histori world .',\n",
       " 'page ?',\n",
       " 'make page , go refer prosper ?',\n",
       " '’ need give vision ambit .']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With the help of snowball stemmer \n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[snowball.stem(word) for word in words if word not in set(stopwords.words('English'))]\n",
    "    sentences[i]=' '.join(words) ## converting all the list of words into the sentences\n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93926ce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pm hegd .',\n",
       " 'okay .',\n",
       " ', first virtu genero , second virtu pure ethic .',\n",
       " 'third toler ’ midst pain , doctor nur , alway midst pain .',\n",
       " '’ toler .',\n",
       " 'fourth persev .',\n",
       " 'doctor never give .',\n",
       " 'never give .',\n",
       " 'alway say , save situat save , prevent disea cure disea .',\n",
       " 'fifth cultiv pure concentr .',\n",
       " 'see patient , feel doctor see .',\n",
       " 'see , keep thermoscop get away .',\n",
       " 'way treat patient , feel , cultiv pure concentr patient .',\n",
       " 'last virtu intellig .',\n",
       " 'mean field countri , part world , need updat .',\n",
       " 'becaus medic field equip , diagnosi , treatment , continu , everyday chang .',\n",
       " 'fortun , internet .',\n",
       " 'virtu empow caregiv human heart .',\n",
       " '’ sure medic present elsewh six virtu reinforc confid citizen healthcar system .',\n",
       " ', final , conclus , would like say rememb , rememb .',\n",
       " 'first , friend , want leav think today .',\n",
       " 'one action make great ?',\n",
       " 'everi one page histori world .',\n",
       " 'page ?',\n",
       " 'make page , go refer prosper ?',\n",
       " '’ need give vision ambit .']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## with the help of lemmatization \n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word,pos='v') for word in words if word not in set(stopwords.words('English'))]\n",
    "    sentences[i]=' '.join(words) ## converting all the list of words into the sentences\n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a9a31a",
   "metadata": {},
   "source": [
    " As we can see that among three , lemmatization gives pretty amazing result when it comes to text preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f231f4",
   "metadata": {},
   "source": [
    "## Parts Of Speech Tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ec2162c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PM Hegde.',\n",
       " 'Okay.',\n",
       " 'So, first virtue is generosity, the second virtue is Pure Ethics.',\n",
       " 'Third is tolerance because you’ll be in the midst of pain, your doctor or nurses, they are always in midst of pain.',\n",
       " 'That’s why tolerance.',\n",
       " 'The fourth is Perseverance.',\n",
       " 'A doctor never gives up.',\n",
       " 'Never gives up.',\n",
       " 'He will always say, I can save this situation or save, prevent the disease or cure the disease.',\n",
       " 'Fifth is Cultivating Pure Concentration.',\n",
       " 'When you see a patient, he should feel my doctor sees me.',\n",
       " 'Sees me through, not just keep the thermoscope and get away.',\n",
       " 'And the way you treat the patient, he feels in this, cultivating the pure concentration of the patient.',\n",
       " 'The last virtue is Intelligent.',\n",
       " 'That means no field in the country, in any part of the world, it needs update.',\n",
       " 'Because medical field and equipment, in diagnosis, in treatment, continuously, everyday its changing.',\n",
       " 'Fortunately, internet is there.',\n",
       " 'These virtues will empower the caregivers with a human heart.',\n",
       " 'I’m sure the medical who are present here and elsewhere will have all these six virtues that will reinforce confidence of the citizen in the healthcare system.',\n",
       " 'So, finally, in conclusion, I would to like say what I will be remembered for, what I will be remembered for.',\n",
       " 'First, friends, I want to leave you with a thought today.',\n",
       " 'What is the one action which will make you great?',\n",
       " 'Every one of you has a page in the history of the world.',\n",
       " 'What is that page?',\n",
       " 'How do you make that page, which is going to be referred by the prosperity?',\n",
       " 'There’s a need to give a vision to your ambition.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentences=nltk.sent_tokenize(Speech)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57fc8ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd007fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PM', 'NNP'), ('Hegde', 'NNP'), ('.', '.')]\n",
      "[('Okay', 'NNP'), ('.', '.')]\n",
      "[('So', 'RB'), (',', ','), ('first', 'JJ'), ('virtue', 'NN'), ('generosity', 'NN'), (',', ','), ('second', 'JJ'), ('virtue', 'NN'), ('Pure', 'NNP'), ('Ethics', 'NNP'), ('.', '.')]\n",
      "[('Third', 'JJ'), ('tolerance', 'NN'), ('’', 'NNP'), ('midst', 'NN'), ('pain', 'NN'), (',', ','), ('doctor', 'NN'), ('nurses', 'NNS'), (',', ','), ('always', 'RB'), ('midst', 'NN'), ('pain', 'NN'), ('.', '.')]\n",
      "[('That', 'DT'), ('’', 'VBD'), ('tolerance', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('fourth', 'JJ'), ('Perseverance', 'NNP'), ('.', '.')]\n",
      "[('A', 'DT'), ('doctor', 'NN'), ('never', 'RB'), ('gives', 'VBZ'), ('.', '.')]\n",
      "[('Never', 'RB'), ('gives', 'VBZ'), ('.', '.')]\n",
      "[('He', 'PRP'), ('always', 'RB'), ('say', 'VBP'), (',', ','), ('I', 'PRP'), ('save', 'VBP'), ('situation', 'NN'), ('save', 'NN'), (',', ','), ('prevent', 'JJ'), ('disease', 'NN'), ('cure', 'NN'), ('disease', 'NN'), ('.', '.')]\n",
      "[('Fifth', 'NNP'), ('Cultivating', 'NNP'), ('Pure', 'NNP'), ('Concentration', 'NNP'), ('.', '.')]\n",
      "[('When', 'WRB'), ('see', 'NN'), ('patient', 'NN'), (',', ','), ('feel', 'VB'), ('doctor', 'NN'), ('sees', 'NNS'), ('.', '.')]\n",
      "[('Sees', 'NNS'), (',', ','), ('keep', 'VB'), ('thermoscope', 'NN'), ('get', 'VB'), ('away', 'RB'), ('.', '.')]\n",
      "[('And', 'CC'), ('way', 'NN'), ('treat', 'NN'), ('patient', 'NN'), (',', ','), ('feels', 'NNS'), (',', ','), ('cultivating', 'VBG'), ('pure', 'JJ'), ('concentration', 'NN'), ('patient', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('last', 'JJ'), ('virtue', 'NN'), ('Intelligent', 'NNP'), ('.', '.')]\n",
      "[('That', 'DT'), ('means', 'VBZ'), ('field', 'NN'), ('country', 'NN'), (',', ','), ('part', 'NN'), ('world', 'NN'), (',', ','), ('needs', 'VBZ'), ('update', 'JJ'), ('.', '.')]\n",
      "[('Because', 'IN'), ('medical', 'JJ'), ('field', 'NN'), ('equipment', 'NN'), (',', ','), ('diagnosis', 'NN'), (',', ','), ('treatment', 'NN'), (',', ','), ('continuously', 'RB'), (',', ','), ('everyday', 'JJ'), ('changing', 'NN'), ('.', '.')]\n",
      "[('Fortunately', 'RB'), (',', ','), ('internet', 'NN'), ('.', '.')]\n",
      "[('These', 'DT'), ('virtues', 'NNS'), ('empower', 'VBP'), ('caregivers', 'NNS'), ('human', 'JJ'), ('heart', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('’', 'VBP'), ('sure', 'JJ'), ('medical', 'JJ'), ('present', 'NN'), ('elsewhere', 'RB'), ('six', 'CD'), ('virtues', 'NNS'), ('reinforce', 'VBP'), ('confidence', 'NN'), ('citizen', 'NN'), ('healthcare', 'NN'), ('system', 'NN'), ('.', '.')]\n",
      "[('So', 'RB'), (',', ','), ('finally', 'RB'), (',', ','), ('conclusion', 'NN'), (',', ','), ('I', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('say', 'VB'), ('I', 'PRP'), ('remembered', 'VBD'), (',', ','), ('I', 'PRP'), ('remembered', 'VBD'), ('.', '.')]\n",
      "[('First', 'RB'), (',', ','), ('friends', 'NNS'), (',', ','), ('I', 'PRP'), ('want', 'VBP'), ('leave', 'JJ'), ('thought', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
      "[('What', 'WP'), ('one', 'CD'), ('action', 'NN'), ('make', 'VB'), ('great', 'JJ'), ('?', '.')]\n",
      "[('Every', 'DT'), ('one', 'CD'), ('page', 'NN'), ('history', 'NN'), ('world', 'NN'), ('.', '.')]\n",
      "[('What', 'WP'), ('page', 'NN'), ('?', '.')]\n",
      "[('How', 'WRB'), ('make', 'JJ'), ('page', 'NN'), (',', ','), ('going', 'VBG'), ('referred', 'VBN'), ('prosperity', 'NN'), ('?', '.')]\n",
      "[('There', 'EX'), ('’', 'NNP'), ('need', 'VBP'), ('give', 'JJ'), ('vision', 'NN'), ('ambition', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## We will find the POS tags \n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[word for word in words if word not in set(stopwords.words('English'))]\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9618a",
   "metadata": {},
   "source": [
    "## Name Entity Recognition \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa2309a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence=\"In September 2023, the United Nations hosted a climate summit in New York City, where leaders like Joe Biden, the President of the United States, and Angela Merkel, the former Chancellor of Germany, discussed the impact of climate change on countries such as India and Australia. During the summit, Microsoft announced a $10 billion investment in renewable energy projects across Africa, aiming to achieve carbon neutrality by 2030. Additionally, NASA presented its latest findings on rising sea levels in the Pacific Ocean, gathered from its Artemis II mission, which was launched from Cape Canaveral.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56564a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "word=nltk.word_tokenize(Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d851b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=nltk.pos_tag(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96c772c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "nltk.download('words')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f2f079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8afb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e99ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
